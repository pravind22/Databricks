{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3940837-ca99-4041-b596-f0f2a7237dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", 1500, \"2023-05-15\"),\n",
    "    (2, \"Bob\", 500, \"2023-02-20\"),\n",
    "    (2, \"Bob\", 700, \"2023-04-22\"),\n",
    "    (3, \"Charlie\", 1200, \"2022-12-10\"),\n",
    "    (4, \"Donald\", 1200, \"2024-12-10\"),\n",
    "    (4, \"Donald\", 1400, \"2024-11-10\"),\n",
    "    (5, \"Tom\", 800, \"2023-12-08\")\n",
    "]\n",
    "\n",
    "schema = ['id', 'name', 'purchase_amount', 'purchase_date']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a820d7-5afa-416f-a247-5ee13a4fda42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1ec828-8218-41c2-875a-0a61399c9757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df1 = df.filter(year(col('purchase_date')) == year(now())-1).filter(col('purchase_amount')>=1000).select('id', 'name').distinct()\n",
    "df1.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4724447c-6c9f-483c-ae59-d56e096a34ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (111,'apple'),\n",
    "    (111,'banana'),\n",
    "    (123,'apple'),\n",
    "    (123,'orange'),\n",
    "    (124,'banana')\n",
    "]\n",
    "\n",
    "schema = ['id', 'name']\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7bd8f04-3980-4089-bc3a-28eb2181d43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, array_contains, size, col\n",
    "\n",
    "result = (\n",
    "    df.groupBy('id')\n",
    "    .agg(\n",
    "        collect_set('name').alias('fruits')\n",
    "    )\n",
    "    .filter(\n",
    "        array_sort(col(\"fruits\"))==array(lit(\"apple\"),lit(\"banana\"))\n",
    "    )\n",
    "    .select('id')\n",
    ")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0c3f78-6620-42ad-a698-ab93da51302b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "from datetime import date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JobRunsExample\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"job_status\", StringType(), nullable=False),\n",
    "    StructField(\"run_date\", DateType(), nullable=False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"success\", date(2025, 8, 1)),\n",
    "    (\"success\", date(2025, 8, 2)),\n",
    "    (\"fail\", date(2025, 8, 3)),\n",
    "    (\"fail\", date(2025, 8, 4)),\n",
    "    (\"success\", date(2025, 8, 13))\n",
    "]\n",
    "\n",
    "df_job_runs = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f8949d-63d9-4a7a-84cc-caa5d108f093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "job_status_window = Window.partitionBy(col('job_status')).orderBy('run_date')\n",
    "window_all = Window.orderBy(col('run_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15dcf168-8a9f-493a-b7aa-af2e0f4b51e9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765946492093}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "job_status_window = Window.partitionBy(col('job_status')).orderBy('run_date')\n",
    "window_all = Window.orderBy(col('run_date'))\n",
    "\n",
    "df_final = (\n",
    "    df_job_runs\n",
    "    .withColumn('r1', row_number().over(job_status_window))\n",
    "    .withColumn('r2', row_number().over(window_all))\n",
    ")\n",
    "\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dca41ce-8af8-4a70-abd5-0b740bcf70ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Coding",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
